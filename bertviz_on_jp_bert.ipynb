{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from bertviz.attention_details import AttentionDetailsData, show\n",
    "from bertviz.pytorch_pretrained_bert import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_file /Users/bayartsogtyadamsuren/Downloads/bert-japanese-files/bert-wiki-ja/wiki-ja.model\n",
      "!!!the path to the vocab file is in SENTENCE PIECE!!! /Users/bayartsogtyadamsuren/Downloads/bert-japanese-files/bert-wiki-ja/wiki-ja.vocab\n",
      "Loaded a trained SentencePiece model.\n"
     ]
    }
   ],
   "source": [
    "bert_version = '/Users/bayartsogtyadamsuren/Downloads/bert-japanese-files/bert-wiki-ja'\n",
    "model = BertModel.from_pretrained(bert_version)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_attention_details(tokens_a, tokens_b, query_vectors, key_vectors, atts):\n",
    "    key_vectors_dict = defaultdict(list)\n",
    "    query_vectors_dict = defaultdict(list)\n",
    "    atts_dict = defaultdict(list)\n",
    "\n",
    "    slice_a = slice(0, len(tokens_a))  # Positions corresponding to sentence A in input\n",
    "    slice_b = slice(len(tokens_a), len(tokens_a) + len(tokens_b))  # Position corresponding to sentence B in input\n",
    "    \n",
    "    num_layers = len(query_vectors)\n",
    "    for layer in range(num_layers):\n",
    "        # Process queries and keys\n",
    "        query_vector = query_vectors[layer][0] # assume batch_size=1; shape = [num_heads, seq_len, vector_size]\n",
    "        key_vector = key_vectors[layer][0] # assume batch_size=1; shape = [num_heads, seq_len, vector_size]\n",
    "        query_vectors_dict['all'].append(query_vector.tolist())\n",
    "        key_vectors_dict['all'].append(key_vector.tolist())\n",
    "        query_vectors_dict['a'].append(query_vector[:, slice_a, :].tolist())\n",
    "        key_vectors_dict['a'].append(key_vector[:, slice_a, :].tolist())\n",
    "        query_vectors_dict['b'].append(query_vector[:, slice_b, :].tolist())\n",
    "        key_vectors_dict['b'].append(key_vector[:, slice_b, :].tolist())\n",
    "        # Process attention\n",
    "        att = atts[layer][0] # assume batch_size=1; shape = [num_heads, source_seq_len, target_seq_len]\n",
    "        atts_dict['all'].append(att.tolist())\n",
    "        atts_dict['aa'].append(att[:, slice_a, slice_a].tolist()) # Append A->A attention for layer, across all heads\n",
    "        atts_dict['bb'].append(att[:, slice_b, slice_b].tolist()) # Append B->B attention for layer, across all heads\n",
    "        atts_dict['ab'].append(att[:, slice_a, slice_b].tolist()) # Append A->B attention for layer, across all heads\n",
    "        atts_dict['ba'].append(att[:, slice_b, slice_a].tolist()) # Append B->A attention for layer, across all heads\n",
    "\n",
    "    attentions =  {\n",
    "        'all': {\n",
    "            'queries': query_vectors_dict['all'],\n",
    "            'keys': key_vectors_dict['all'],\n",
    "            'att': atts_dict['all'],\n",
    "            'left_text': tokens_a + tokens_b,\n",
    "            'right_text': tokens_a + tokens_b\n",
    "        },\n",
    "        'aa': {\n",
    "            'queries': query_vectors_dict['a'],\n",
    "            'keys': key_vectors_dict['a'],\n",
    "            'att': atts_dict['aa'],\n",
    "            'left_text': tokens_a,\n",
    "            'right_text': tokens_a\n",
    "        },\n",
    "        'bb': {\n",
    "            'queries': query_vectors_dict['b'],\n",
    "            'keys': key_vectors_dict['b'],\n",
    "            'att': atts_dict['bb'],\n",
    "            'left_text': tokens_b,\n",
    "            'right_text': tokens_b\n",
    "        },\n",
    "        'ab': {\n",
    "            'queries': query_vectors_dict['a'],\n",
    "            'keys': key_vectors_dict['b'],\n",
    "            'att': atts_dict['ab'],\n",
    "            'left_text': tokens_a,\n",
    "            'right_text': tokens_b\n",
    "        },\n",
    "        'ba': {\n",
    "            'queries': query_vectors_dict['b'],\n",
    "            'keys': key_vectors_dict['a'],\n",
    "            'att': atts_dict['ba'],\n",
    "            'left_text': tokens_b,\n",
    "            'right_text': tokens_a\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showComputation(config):\n",
    "#     print(\"attention\",config[\"attention\"])\n",
    "    att_dets = config[\"attention\"][config[\"att_type\"]]\n",
    "    query_vector = att_dets[\"queries\"][config[\"layer\"]][config[\"att_head\"]][config[\"query_index\"]]\n",
    "    keys = att_dets[\"keys\"][config[\"layer\"]][config[\"att_head\"]]\n",
    "    att = att_dets[\"att\"][config[\"layer\"]][config[\"att_head\"]][config[\"query_index\"]]\n",
    "    \n",
    "    seq_len = len(keys)\n",
    "    dotProducts = []\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        key_vector = keys[i]\n",
    "        dotProduct = 0\n",
    "        \n",
    "        for j in range(config[\"vector_size\"]):\n",
    "            product = query_vector[j] * key_vector[j]\n",
    "            dotProduct += product\n",
    "        dotProducts.append(dotProduct)\n",
    "    \n",
    "    return dotProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    article_id     title  \\\n",
      "0  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "1  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "2  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "3  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "4  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "\n",
      "                                                text  \n",
      "0  ごく普通のスポーツバッグかと思ったら保冷バッグと知り、デザイン性の高さに驚きました。ジムバッ...  \n",
      "1  秋に向けて大活躍しそうなDEAN &amp; DELUCAのスープポットが9月1日に新発売！...  \n",
      "2  マスキングテープといえば、我が家のラインナップは「用途を選ばず便利」という理由で無地のものば...  \n",
      "3  週末は、冷えたビールにアツアツの餃子が定番の我が家。そんな餃子大好きな私が見つけてしまったの...  \n",
      "4  かさもアップ。常にテーブルにお花がある生活って素敵ですよね。でもズボラな私はなかなか毎日飾る...  \n",
      "154\n"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(\"/Users/bayartsogtyadamsuren/Downloads/bert-japanese-files/bertviz_samples/bertviz_input_chosen_jp.csv\")\n",
    "print(f.head())\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.96s/it]\u001b[A\n",
      "2it [00:06,  2.59s/it]\u001b[A\n",
      "3it [00:07,  2.39s/it]\u001b[A\n",
      "4it [00:10,  2.56s/it]\u001b[A\n",
      "5it [00:13,  2.67s/it]\u001b[A\n",
      "6it [00:16,  2.76s/it]\u001b[A\n",
      "7it [00:18,  2.51s/it]\u001b[A\n",
      "8it [00:20,  2.34s/it]\u001b[A\n",
      "9it [00:21,  1.86s/it]\u001b[A\n",
      "10it [00:22,  1.57s/it]\u001b[A\n",
      "11it [00:24,  1.80s/it]\u001b[A\n",
      "12it [00:25,  1.44s/it]\u001b[A\n",
      "13it [00:28,  1.90s/it]\u001b[A\n",
      "14it [00:28,  1.47s/it]\u001b[A\n",
      "15it [00:29,  1.26s/it]\u001b[A\n",
      "16it [00:30,  1.31s/it]\u001b[A\n",
      "17it [00:31,  1.03s/it]\u001b[A\n",
      "18it [00:31,  1.15it/s]\u001b[A\n",
      "19it [00:33,  1.20s/it]\u001b[A\n",
      "20it [00:35,  1.23s/it]\u001b[A\n",
      "21it [00:36,  1.17s/it]\u001b[A\n",
      "22it [00:39,  1.97s/it]\u001b[A\n",
      "23it [00:40,  1.54s/it]\u001b[A\n",
      "24it [00:42,  1.59s/it]\u001b[A\n",
      "25it [00:43,  1.49s/it]\u001b[A\n",
      "26it [00:44,  1.30s/it]\u001b[A\n",
      "27it [00:44,  1.04s/it]\u001b[A\n",
      "28it [00:47,  1.48s/it]\u001b[A\n",
      "29it [00:47,  1.26s/it]\u001b[A\n",
      "30it [00:48,  1.06s/it]\u001b[A\n",
      "31it [00:49,  1.08s/it]\u001b[A\n",
      "32it [00:50,  1.02it/s]\u001b[A\n",
      "33it [00:52,  1.23s/it]\u001b[A\n",
      "34it [00:53,  1.18s/it]\u001b[A\n",
      "35it [00:54,  1.27s/it]\u001b[A\n",
      "36it [00:56,  1.39s/it]\u001b[A\n",
      "37it [00:57,  1.43s/it]\u001b[A\n",
      "38it [00:58,  1.24s/it]\u001b[A\n",
      "39it [00:59,  1.02it/s]\u001b[A\n",
      "40it [01:00,  1.01it/s]\u001b[A\n",
      "41it [01:01,  1.10s/it]\u001b[A\n",
      "42it [01:02,  1.14s/it]\u001b[A\n",
      "43it [01:03,  1.11it/s]\u001b[A\n",
      "44it [01:04,  1.04s/it]\u001b[A\n",
      "45it [01:06,  1.20s/it]\u001b[A\n",
      "46it [01:06,  1.01s/it]\u001b[A\n",
      "47it [01:07,  1.19it/s]\u001b[A\n",
      "48it [01:09,  1.30s/it]\u001b[A\n",
      "49it [01:10,  1.17s/it]\u001b[A\n",
      "50it [01:11,  1.22s/it]\u001b[A\n",
      "51it [01:13,  1.38s/it]\u001b[A\n",
      "52it [01:14,  1.26s/it]\u001b[A\n",
      "53it [01:15,  1.20s/it]\u001b[A\n",
      "54it [01:17,  1.41s/it]\u001b[A\n",
      "55it [01:17,  1.18s/it]\u001b[A\n",
      "56it [01:19,  1.34s/it]\u001b[A\n",
      "57it [01:26,  2.86s/it]\u001b[A\n",
      "58it [01:27,  2.52s/it]\u001b[A\n",
      "59it [01:28,  2.00s/it]\u001b[A\n",
      "60it [01:28,  1.51s/it]\u001b[A\n",
      "61it [01:29,  1.17s/it]\u001b[A\n",
      "62it [01:29,  1.06it/s]\u001b[A\n",
      "63it [01:29,  1.37it/s]\u001b[A\n",
      "64it [01:30,  1.45it/s]\u001b[A\n",
      "65it [01:31,  1.58it/s]\u001b[A\n",
      "66it [01:31,  1.80it/s]\u001b[A\n",
      "67it [01:32,  1.57it/s]\u001b[A\n",
      "68it [01:32,  1.82it/s]\u001b[A\n",
      "69it [01:33,  1.98it/s]\u001b[A\n",
      "70it [01:33,  2.31it/s]\u001b[A\n",
      "71it [01:33,  2.58it/s]\u001b[A\n",
      "72it [01:34,  1.55it/s]\u001b[A\n",
      "73it [01:35,  1.77it/s]\u001b[A\n",
      "74it [01:37,  1.05s/it]\u001b[A\n",
      "75it [01:39,  1.31s/it]\u001b[A\n",
      "76it [01:41,  1.69s/it]\u001b[A\n",
      "77it [01:45,  2.24s/it]\u001b[A\n",
      "78it [01:47,  2.25s/it]\u001b[A\n",
      "79it [01:50,  2.34s/it]\u001b[A\n",
      "80it [01:52,  2.21s/it]\u001b[A\n",
      "81it [01:53,  2.05s/it]\u001b[A\n",
      "82it [01:55,  1.90s/it]\u001b[A\n",
      "83it [01:56,  1.61s/it]\u001b[A\n",
      "84it [02:02,  2.98s/it]\u001b[A\n",
      "85it [02:06,  3.39s/it]\u001b[A\n",
      "86it [02:07,  2.63s/it]\u001b[A\n",
      "87it [02:12,  3.23s/it]\u001b[A\n",
      "88it [02:12,  2.38s/it]\u001b[A\n",
      "89it [02:13,  1.97s/it]\u001b[A\n",
      "90it [02:19,  3.16s/it]\u001b[A\n",
      "91it [02:20,  2.57s/it]\u001b[A\n",
      "92it [02:21,  1.99s/it]\u001b[A\n",
      "93it [02:24,  2.22s/it]\u001b[A\n",
      "94it [02:47,  8.46s/it]\u001b[A\n",
      "95it [02:49,  6.49s/it]\u001b[A\n",
      "96it [02:51,  5.37s/it]\u001b[A\n",
      "97it [02:52,  3.87s/it]\u001b[A\n",
      "98it [02:52,  2.83s/it]\u001b[A\n",
      "99it [02:54,  2.41s/it]\u001b[A\n",
      "100it [02:54,  1.84s/it]\u001b[A\n",
      "101it [02:55,  1.64s/it]\u001b[A\n",
      "102it [02:58,  1.96s/it]\u001b[A\n",
      "103it [02:59,  1.63s/it]\u001b[A\n",
      "104it [02:59,  1.33s/it]\u001b[A\n",
      "105it [03:00,  1.22s/it]\u001b[A\n",
      "106it [03:01,  1.17s/it]\u001b[A\n",
      "107it [03:03,  1.21s/it]\u001b[A\n",
      "108it [03:04,  1.20s/it]\u001b[A\n",
      "109it [03:07,  1.72s/it]\u001b[A\n",
      "110it [03:08,  1.66s/it]\u001b[A\n",
      "111it [03:10,  1.53s/it]\u001b[A\n",
      "112it [03:11,  1.34s/it]\u001b[A\n",
      "113it [03:12,  1.24s/it]\u001b[A\n",
      "114it [03:13,  1.19s/it]\u001b[A\n",
      "115it [03:15,  1.69s/it]\u001b[A\n",
      "116it [03:16,  1.39s/it]\u001b[A\n",
      "117it [03:18,  1.56s/it]\u001b[A\n",
      "118it [03:23,  2.43s/it]\u001b[A\n",
      "119it [03:49,  9.69s/it]\u001b[A\n",
      "120it [04:23, 16.86s/it]\u001b[A\n",
      "121it [04:53, 20.73s/it]\u001b[A\n",
      "122it [05:20, 22.89s/it]\u001b[A\n",
      "123it [05:25, 17.40s/it]\u001b[A\n",
      "124it [05:27, 12.61s/it]\u001b[A\n",
      "125it [05:28,  9.20s/it]\u001b[A\n",
      "126it [05:29,  6.88s/it]\u001b[A\n",
      "127it [05:41,  8.42s/it]\u001b[A\n",
      "128it [05:46,  7.19s/it]\u001b[A\n",
      "129it [05:53,  7.42s/it]\u001b[A\n",
      "130it [05:55,  5.60s/it]\u001b[A\n",
      "131it [06:04,  6.61s/it]\u001b[A\n",
      "132it [06:07,  5.48s/it]\u001b[A\n",
      "133it [06:14,  6.09s/it]\u001b[A\n",
      "134it [06:22,  6.76s/it]\u001b[A\n",
      "135it [06:28,  6.48s/it]\u001b[A\n",
      "136it [06:33,  6.05s/it]\u001b[A\n",
      "137it [06:39,  6.05s/it]\u001b[A\n",
      "138it [06:45,  5.81s/it]\u001b[A\n",
      "139it [06:47,  4.69s/it]\u001b[A\n",
      "140it [06:47,  3.43s/it]\u001b[A\n",
      "141it [06:50,  3.13s/it]\u001b[A\n",
      "142it [06:50,  2.38s/it]\u001b[A\n",
      "143it [06:53,  2.45s/it]\u001b[A\n",
      "144it [06:54,  1.90s/it]\u001b[A\n",
      "145it [06:56,  2.07s/it]\u001b[A\n",
      "146it [06:57,  1.61s/it]\u001b[A\n",
      "147it [06:58,  1.63s/it]\u001b[A\n",
      "148it [07:00,  1.76s/it]\u001b[A\n",
      "149it [07:02,  1.84s/it]\u001b[A\n",
      "150it [07:03,  1.60s/it]\u001b[A\n",
      "151it [07:08,  2.49s/it]\u001b[A\n",
      "152it [07:08,  1.89s/it]\u001b[A\n",
      "153it [07:11,  2.16s/it]\u001b[A\n",
      "154it [07:12,  1.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Too Longs:  3\n"
     ]
    }
   ],
   "source": [
    "q_x_k_scores = []\n",
    "para_tokens = []\n",
    "too_long = 0\n",
    "errors = 0\n",
    "\n",
    "ff = open(\"/Users/bayartsogtyadamsuren/Downloads/bert-japanese-files/bertviz_samples/bertviz_input_chosen_jp_token2token.tsv\", \"w\")\n",
    "ff.write(\"id\\ttitle token\\ttext token\\tscore\\n\")\n",
    "\n",
    "for i, x in tqdm(f.iterrows()):\n",
    "    \n",
    "    sentence_a = str(x[\"text\"]).replace(\"\\n\",\"。\").replace(\"〝\",\"\").replace(\"〞\",\"\").replace(\"「\",\"\").replace(\"」\",\"\").strip()\n",
    "    sentence_b = x[\"title\"].replace(\"\\n\",\"\").replace(\"〝\",\"\").replace(\"〞\",\"\").replace(\"「\",\"\").replace(\"」\",\"\").strip()\n",
    "    \n",
    "    if len (sentence_a) > 512 or len (sentence_a) > 512:\n",
    "        too_long += 1\n",
    "        sentence_a = sentence_a[:512]\n",
    "        sentence_b = sentence_b[:512]\n",
    "#         raise Exception(\"too long\")\n",
    "    \n",
    "    details_data = AttentionDetailsData(model, tokenizer)\n",
    "    tokens_a, tokens_b, queries, keys, atts = details_data.get_data(sentence_a, sentence_b)\n",
    "    attentions = _get_attention_details(tokens_a, tokens_b, queries, keys, atts)\n",
    "    q_x_k_score = np.zeros((len(tokens_a),))\n",
    "\n",
    "    for j, k in enumerate(tokens_b):\n",
    "\n",
    "        config = {\n",
    "            \"attention\": attentions,\n",
    "            \"att_type\": \"ba\",\n",
    "            \"vector_size\": 64,\n",
    "            \"layer\": 9,\n",
    "            \"att_head\": 6,\n",
    "            \"query_index\": j\n",
    "        }\n",
    "        q_x_k_score += np.array(showComputation(config))\n",
    "        \n",
    "        ## token2token code change\n",
    "        for j in range(len(tokens_a)):\n",
    "            ff.write(f\"{x['article_id']}\\t{k}\\t{tokens_a[j]}\\t{showComputation(config)[j]}\\n\")\n",
    "                     \n",
    "    assert len(q_x_k_score) == len(tokens_a)\n",
    "        \n",
    "    q_x_k_scores.append(q_x_k_score)\n",
    "    para_tokens.append(tokens_a)\n",
    "#     break\n",
    "ff.close()\n",
    "print(\"Total Too Longs: \", too_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    かわいいNEWS\n",
       "1    かわいいNEWS\n",
       "2    かわいいNEWS\n",
       "3    かわいいNEWS\n",
       "4    かわいいNEWS\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[\"title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_tokens_ = [\",\".join(x) for x in para_tokens]\n",
    "q_x_k_scores_ = [\",\".join([str(l) for l in list(x)]) for x in q_x_k_scores]\n",
    "f[\"paragraph_tokens\"] = para_tokens_\n",
    "f[\"q*k\"] = q_x_k_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    article_id     title  \\\n",
      "0  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "1  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "2  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "3  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "4  schIBJP010822173500_art0001  かわいいNEWS   \n",
      "\n",
      "                                           paragraph  \\\n",
      "0  ごく普通のスポーツバッグかと思ったら保冷バッグと知り、デザイン性の高さに驚きました。ジムバッ...   \n",
      "1  秋に向けて大活躍しそうなDEAN &amp; DELUCAのスープポットが9月1日に新発売！...   \n",
      "2  マスキングテープといえば、我が家のラインナップは「用途を選ばず便利」という理由で無地のものば...   \n",
      "3  週末は、冷えたビールにアツアツの餃子が定番の我が家。そんな餃子大好きな私が見つけてしまったの...   \n",
      "4  かさもアップ。常にテーブルにお花がある生活って素敵ですよね。でもズボラな私はなかなか毎日飾る...   \n",
      "\n",
      "                                    paragraph_tokens  \\\n",
      "0  [CLS],▁,ごく,普通の,スポーツ,バッグ,か,と思った,ら,保,冷,バッグ,と,知り,...   \n",
      "1  [CLS],▁,秋,に向けて,大,活躍し,そうな,de,an,▁&,amp,;,▁de,lu...   \n",
      "2  [CLS],▁,マス,キング,テープ,といえば,、,我が,家の,ラインナップ,は,用途,を,...   \n",
      "3  [CLS],▁,週末,は,、,冷,えた,ビール,に,ア,ツ,ア,ツ,の,餃,子が,定番,の,...   \n",
      "4  [CLS],▁,かさ,も,アップ,。,常に,テーブル,に,お,花,がある,生活,って,素,敵...   \n",
      "\n",
      "                                                 q*k  \n",
      "0  110.55391529070833,115.06578941142755,-8.67406...  \n",
      "1  113.93482727274728,114.10239268376226,129.7944...  \n",
      "2  56.445613481652494,126.40534373321229,82.30859...  \n",
      "3  53.238954647136055,112.03032352120688,111.8142...  \n",
      "4  94.78399916033817,89.06140743303354,102.265414...  \n"
     ]
    }
   ],
   "source": [
    "print(f.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paragraph_tokens</th>\n",
       "      <th>q*k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>かわいいNEWS</td>\n",
       "      <td>[CLS],▁,ごく,普通の,スポーツ,バッグ,か,と思った,ら,保,冷,バッグ,と,知り,...</td>\n",
       "      <td>110.55391529070833,115.06578941142755,-8.67406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>かわいいNEWS</td>\n",
       "      <td>[CLS],▁,秋,に向けて,大,活躍し,そうな,de,an,▁&amp;,amp,;,▁de,lu...</td>\n",
       "      <td>113.93482727274728,114.10239268376226,129.7944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>かわいいNEWS</td>\n",
       "      <td>[CLS],▁,マス,キング,テープ,といえば,、,我が,家の,ラインナップ,は,用途,を,...</td>\n",
       "      <td>56.445613481652494,126.40534373321229,82.30859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>かわいいNEWS</td>\n",
       "      <td>[CLS],▁,週末,は,、,冷,えた,ビール,に,ア,ツ,ア,ツ,の,餃,子が,定番,の,...</td>\n",
       "      <td>53.238954647136055,112.03032352120688,111.8142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>かわいいNEWS</td>\n",
       "      <td>[CLS],▁,かさ,も,アップ,。,常に,テーブル,に,お,花,がある,生活,って,素,敵...</td>\n",
       "      <td>94.78399916033817,89.06140743303354,102.265414...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title                                   paragraph_tokens  \\\n",
       "0  かわいいNEWS  [CLS],▁,ごく,普通の,スポーツ,バッグ,か,と思った,ら,保,冷,バッグ,と,知り,...   \n",
       "1  かわいいNEWS  [CLS],▁,秋,に向けて,大,活躍し,そうな,de,an,▁&,amp,;,▁de,lu...   \n",
       "2  かわいいNEWS  [CLS],▁,マス,キング,テープ,といえば,、,我が,家の,ラインナップ,は,用途,を,...   \n",
       "3  かわいいNEWS  [CLS],▁,週末,は,、,冷,えた,ビール,に,ア,ツ,ア,ツ,の,餃,子が,定番,の,...   \n",
       "4  かわいいNEWS  [CLS],▁,かさ,も,アップ,。,常に,テーブル,に,お,花,がある,生活,って,素,敵...   \n",
       "\n",
       "                                                 q*k  \n",
       "0  110.55391529070833,115.06578941142755,-8.67406...  \n",
       "1  113.93482727274728,114.10239268376226,129.7944...  \n",
       "2  56.445613481652494,126.40534373321229,82.30859...  \n",
       "3  53.238954647136055,112.03032352120688,111.8142...  \n",
       "4  94.78399916033817,89.06140743303354,102.265414...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ = f[[\"title\",\"paragraph_tokens\",\"q*k\"]]\n",
    "f_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_.to_csv(\"/Users/bayartsogtyadamsuren/Downloads/bert_viz_samples.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
